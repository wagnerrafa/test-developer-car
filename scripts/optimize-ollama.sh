#!/bin/bash

# Script para otimizar performance do Ollama
echo "ğŸš€ Otimizando performance do Ollama..."

# Verifica se o Ollama estÃ¡ rodando
if ! docker ps | grep -q "base_tests_ollama"; then
    echo "âŒ Container Ollama nÃ£o estÃ¡ rodando. Inicie com: docker-compose -f docker-compose.ollama.yml up -d"
    exit 1
fi

echo "ğŸ“Š Verificando status atual do Ollama..."

# Verifica modelos carregados
echo "ğŸ“‹ Modelos disponÃ­veis:"
docker exec base_tests_ollama ollama list

# Verifica uso de memÃ³ria
echo "ğŸ’¾ Uso de memÃ³ria:"
docker exec base_tests_ollama sh -c "free -h && echo '---' && ps aux --sort=-%mem | head -10"

# OtimizaÃ§Ãµes de sistema
echo "âš¡ Aplicando otimizaÃ§Ãµes de sistema..."

# Aumenta limites de memÃ³ria compartilhada
docker exec base_tests_ollama sh -c "echo 'kernel.shmmax = 2147483648' >> /etc/sysctl.conf" 2>/dev/null || true
docker exec base_tests_ollama sh -c "echo 'kernel.shmall = 524288' >> /etc/sysctl.conf" 2>/dev/null || true

# Otimiza configuraÃ§Ãµes do Ollama
echo "ğŸ”§ Configurando variÃ¡veis de ambiente otimizadas..."
docker exec base_tests_ollama sh -c "
    export OLLAMA_NUM_PARALLEL=4
    export OLLAMA_MAX_LOADED_MODELS=2
    export OLLAMA_MAX_QUEUE=512
    export OLLAMA_KEEP_ALIVE=24h
    export OLLAMA_ORIGINS=*
"

# PrÃ©-carrega o modelo para evitar latÃªncia
echo "ğŸ”„ PrÃ©-carregando modelo para melhor performance..."
docker exec base_tests_ollama ollama run llama3.1:8b "Hello, this is a test to warm up the model." > /dev/null 2>&1

echo "âœ… OtimizaÃ§Ãµes aplicadas!"
echo ""
echo "ğŸ“ˆ Dicas adicionais para melhor performance:"
echo "1. Use modelos menores (7b em vez de 13b) se possÃ­vel"
echo "2. Reduza max_tokens para respostas mais curtas"
echo "3. Use temperature baixa (0.1-0.3) para respostas mais diretas"
echo "4. Considere usar GPU se disponÃ­vel"
echo ""
echo "ğŸ” Para monitorar performance:"
echo "docker exec base_tests_ollama ollama ps"
echo "docker stats base_tests_ollama"
