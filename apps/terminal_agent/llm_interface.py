"""
Interface abstrata para provedores de LLM.

Este m√≥dulo define a interface padr√£o que todos os provedores de LLM devem implementar,
permitindo f√°cil troca entre diferentes modelos e servi√ßos.
"""

import json
import logging
import re
from abc import ABC, abstractmethod
from typing import Any, Optional

logger = logging.getLogger(__name__)


class LLMPrompts:
    """Constantes com prompts padronizados para todos os LLMs."""

    # Prompt para extra√ß√£o de prefer√™ncias
    EXTRACT_PREFERENCES_SYSTEM = """
    Voc√™ √© um assistente especializado em extrair prefer√™ncias de carros de conversas naturais.

    Extraia as seguintes informa√ß√µes do input do usu√°rio:
    - marca: marca do carro (ex: Audi, BMW, Toyota)
    - modelo: modelo espec√≠fico (ex: A4, X5, Corolla)
    - faixa_preco: faixa de pre√ßo (economico, medio, luxo)
    - ano: prefer√™ncia de ano. Se o usu√°rio fornecer um ano espec√≠fico (ex: 2016), retorne um N√öMERO inteiro (ex: 2016). A sa√≠da deve ser sempre um N√öMERO inteiro, qunado solicitado um ano.
    - combustivel: tipo de combust√≠vel (gasolina, etanol, flex, diesel, eletrico, hibrido)
    - transmissao: tipo de transmiss√£o (manual, automatico, cvt, semi_automatico, dual_clutch)
    - cor: cor preferida
    - portas: n√∫mero de portas (2, 4, 5)
    - quilometragem: quilometragem m√°xima aceita
    - uso: tipo de uso (cidade, estrada, trabalho, lazer)

    Responda APENAS com um JSON v√°lido contendo as prefer√™ncias extra√≠das.
    Use null para informa√ß√µes n√£o mencionadas.
    """

    # Prompt para gera√ß√£o de filtros
    GENERATE_FILTERS_SYSTEM = """
    Voc√™ √© um especialista em converter prefer√™ncias de carros em filtros de busca para Django ORM.

    REGRAS IMPORTANTES DE SA√çDA (obrigat√≥rias):
    - Responda APENAS com um JSON V√ÅLIDO (sem explica√ß√µes).
    - Use campos FLAT (planos), SEM operadores como $eq, $gte, $lte, $in.
    - Mapeie ranges para chaves com sufixo _min/_max (ex.: price_min, price_max).
    - Use exatamente estas chaves quando aplic√°vel:
      - brand_name (marca do carro em texto)
      - car_name (nome do carro em texto)
      - color_name, engine_name, car_model_name
      - fuel_type (gasoline, ethanol, flex, diesel, electric, hybrid)
      - transmission (manual, automatic, cvt, semi_automatic, dual_clutch)
      - price_min, price_max
      - year_manufacture_min, year_manufacture_max (se o usu√°rio disser um ano espec√≠fico ex.: 2016, use os dois com o MESMO valor 2016)
      - year_model_min, year_model_max (pode repetir o mesmo valor do ano espec√≠fico quando aplic√°vel)
      - mileage_min, mileage_max
      - doors_min, doors_max
    - N√£o inclua campos com valor null/None.

    Exemplos de sa√≠da CORRETA:
    {"brand_name": "Audi"}
    {"brand_name": "Audi", "price_max": 120000, "year_manufacture_min": 2018}
    """

    # Prompt para formata√ß√£o de resultados
    FORMAT_RESULTS_SYSTEM = """
    Voc√™ √© um assistente virtual especializado em apresentar resultados de busca de carros.

    Apresente os resultados de forma:
    - Amig√°vel e conversacional
    - Destacando caracter√≠sticas importantes
    - Usando emojis apropriados
    - Formato de lista clara
    - Sugerindo pr√≥ximos passos
    - Seja conciso e direto
    """

    # Prompt para gera√ß√£o de perguntas
    GENERATE_QUESTION_SYSTEM = """
    Voc√™ √© um assistente de vendas de carros experiente.

    Gere uma pergunta natural e amig√°vel para esclarecer as prefer√™ncias do cliente.
    Seja conversacional e √∫til, n√£o rob√≥tico.
    Foque em uma informa√ß√£o por vez para n√£o sobrecarregar.
    """

    # Templates de perguntas por tipo de informa√ß√£o
    QUESTION_TEMPLATES = {
        "marca": "Qual marca de carro voc√™ prefere? (ex: Audi, BMW, Toyota, Honda...)",
        "faixa_preco": "Qual sua faixa de pre√ßo preferida? (econ√¥mico, m√©dio, luxo)",
        "ano": "Prefere carros mais novos ou usados?",
        "combustivel": "Qual tipo de combust√≠vel prefere? (gasolina, etanol, flex, h√≠brido...)",
        "transmissao": "Prefere c√¢mbio manual ou autom√°tico?",
        "cor": "Tem alguma cor preferida?",
        "portas": "Quantas portas prefere? (2, 4 ou 5 portas)",
        "quilometragem": "Qual a quilometragem m√°xima que aceita?",
        "uso": "Para que tipo de uso? (cidade, estrada, trabalho, lazer)",
    }

    # Configura√ß√µes de gera√ß√£o
    GENERATION_CONFIGS = {
        "extract_preferences": {"temperature": 0.1, "max_tokens": 200},
        "generate_filters": {"temperature": 0.1, "max_tokens": 200},
        "format_results": {"temperature": 0.5, "max_tokens": 300},
        "generate_question": {"temperature": 0.7, "max_tokens": 150},
    }


class LLMInterface(ABC):
    """Interface abstrata para provedores de LLM."""

    # M√©todos utilit√°rios compartilhados
    def _extract_json_from_response(self, response: str) -> Optional[str]:
        """Extrai JSON de uma resposta de texto de forma robusta."""
        # Procurar por blocos de c√≥digo JSON
        json_pattern = r"```(?:json)?\s*(\{.*?\})\s*```"
        match = re.search(json_pattern, response, re.DOTALL)
        if match:
            return match.group(1)

        # Procurar por JSON simples
        json_start = response.find("{")
        json_end = response.rfind("}") + 1

        if json_start != -1 and json_end > json_start:
            return response[json_start:json_end]

        return None

    def _simplify_cars_for_formatting(self, cars: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """Simplifica estrutura dos carros para formata√ß√£o."""
        simplified_cars = []
        for car in cars:
            try:
                simplified_car = {
                    "marca": car.get("car_name", {}).get("brand", {}).get("name", "N/A"),
                    "modelo": car.get("car_name", {}).get("name", "N/A"),
                    "ano": car.get("year_manufacture", "N/A"),
                    "cor": car.get("color", {}).get("name", "N/A"),
                    "preco": car.get("price", 0),
                    "quilometragem": car.get("mileage", 0),
                    "combustivel": car.get("fuel_type", "N/A"),
                    "transmissao": car.get("transmission", "N/A"),
                }
                simplified_cars.append(simplified_car)
            except Exception as e:
                logger.warning(f"Erro ao simplificar carro: {e}")
                continue

        return simplified_cars

    def _format_cars_simple(self, cars: list[dict[str, Any]]) -> str:
        """Formata√ß√£o simples como fallback."""
        if not cars:
            return "üòî N√£o encontrei carros que atendam aos seus crit√©rios."

        result_text = f"üöó Encontrei {len(cars)} carro(s):\n\n"

        for i, car in enumerate(cars, 1):
            try:
                brand_name = car.get("car_name", {}).get("brand", {}).get("name", "N/A")
                car_name = car.get("car_name", {}).get("name", "N/A")
                year = car.get("year_manufacture", "N/A")
                price = car.get("price", 0)

                result_text += f"{i}. **{brand_name} {car_name} ({year})** - R$ {price:,.2f}\n"
            except Exception as e:
                logger.warning(f"Erro ao formatar carro {i}: {e}")
                continue

        return result_text

    def _validate_preferences(self, preferences: dict[str, Any]) -> dict[str, Any]:
        """Valida e limpa prefer√™ncias extra√≠das."""
        valid_preferences = {}

        # Mapear campos v√°lidos
        field_mapping = {
            "marca": "marca",
            "modelo": "modelo",
            "faixa_preco": "faixa_preco",
            "ano": "ano",
            "combustivel": "combustivel",
            "transmissao": "transmissao",
            "cor": "cor",
            "portas": "portas",
            "quilometragem": "quilometragem",
            "uso": "uso",
        }

        for key, value in preferences.items():
            if key in field_mapping and value is not None:
                valid_preferences[field_mapping[key]] = value

        return valid_preferences

    def _convert_preferences_to_filters(self, preferences: dict[str, Any]) -> dict[str, Any]:
        """Modificar prefer√™ncias em filtros MCP."""
        filters = {
            "brand_name": preferences.get("marca"),
            "price_min": None,
            "price_max": None,
            "year_manufacture_min": None,
            "year_manufacture_max": None,
            "year_model_min": None,
            "year_model_max": None,
            "fuel_type": preferences.get("combustivel"),
            "transmission": preferences.get("transmissao"),
            "color_name": preferences.get("cor"),
            "doors_min": None,
            "doors_max": None,
            "mileage_min": None,
            "mileage_max": None,
        }

        # Aplicar faixa de pre√ßo
        faixa_preco = preferences.get("faixa_preco")
        if faixa_preco == "economico":
            filters["price_max"] = 50000
        elif faixa_preco == "medio":
            filters["price_min"] = 30000
            filters["price_max"] = 100000
        elif faixa_preco == "luxo":
            filters["price_min"] = 100000

        # Aplicar ano: aceitar inteiro espec√≠fico ou categorias
        ano = preferences.get("ano")
        if isinstance(ano, int):
            filters["year_manufacture_min"] = ano
            filters["year_manufacture_max"] = ano
            filters["year_model_min"] = ano
            filters["year_model_max"] = ano
        elif isinstance(ano, str):
            if ano == "recente":
                filters["year_manufacture_min"] = 2020
                filters["year_model_min"] = 2020
            elif ano == "antigo":
                filters["year_manufacture_max"] = 2015
                filters["year_model_max"] = 2015

        # Aplicar quilometragem
        quilometragem = preferences.get("quilometragem")
        if quilometragem and isinstance(quilometragem, (int, float)):
            filters["mileage_max"] = int(quilometragem)

        # Aplicar portas
        portas = preferences.get("portas")
        if portas and isinstance(portas, (int, float)):
            portas_int = int(portas)
            filters["doors_min"] = portas_int
            filters["doors_max"] = portas_int

        # Remover valores None
        return {k: v for k, v in filters.items() if v is not None}

    def _normalize_llm_filters(self, raw_filters: dict[str, Any]) -> dict[str, Any]:
        """
        Normaliza filtros produzidos pela LLM para o formato plano esperado pelos serializers/ORM.

        - Remove operadores como $eq/$gte/$lte/eq/gte/lte, mantendo o valor base
        - Converte dicts de range para *_min/_max
        - Mapeia sin√¥nimos/portugu√™s para chaves esperadas (ex.: marca -> brand_name)
        - Remove valores None/vazios
        """
        if not isinstance(raw_filters, dict):
            return {}

        normalized: dict[str, Any] = {}

        # Mapeamento de sin√¥nimos de chaves
        key_synonyms = {
            "marca": "brand_name",
            "brand": "brand_name",
            "nome_marca": "brand_name",
            "modelo": "car_name",
            "nome_carro": "car_name",
            "cor": "color_name",
            "motor": "engine_name",
            "combustivel": "fuel_type",
            "transmissao": "transmission",
            "preco_min": "price_min",
            "preco_max": "price_max",
            "ano_min": "year_manufacture_min",
            "ano_max": "year_manufacture_max",
            "quilometragem_min": "mileage_min",
            "quilometragem_max": "mileage_max",
            "portas": "doors",  # pode virar doors_min/max abaixo
        }

        def strip_operator(value: Any) -> Any:
            if isinstance(value, dict):
                # Operadores comuns
                for op in ("$eq", "eq", "=", "value"):
                    if op in value:
                        return value[op]
                # Se vier min/max ou gte/lte, retorna o pr√≥prio dict para tratamento ap√≥s
                if any(k in value for k in ("$gte", "gte", "min", "$lte", "lte", "max")):
                    return value
            return value

        # Primeiro, normalizar chaves e tirar operadores triviais ($eq)
        temp: dict[str, Any] = {}
        for k, v in raw_filters.items():
            key = key_synonyms.get(k, k)
            temp[key] = strip_operator(v)

        # Tratar ranges e valores planos
        for key, value in temp.items():
            if value is None or value == "":
                continue

            # Ranges: price/year/doors/mileage
            if isinstance(value, dict):
                # Mapear poss√≠veis chaves
                min_val = None
                max_val = None
                if "$gte" in value or "gte" in value:
                    min_val = value.get("$gte", value.get("gte"))
                if "$lte" in value or "lte" in value:
                    max_val = value.get("$lte", value.get("lte"))
                if "min" in value:
                    min_val = value.get("min") if min_val is None else min_val
                if "max" in value:
                    max_val = value.get("max") if max_val is None else max_val

                if key in ("price", "year_manufacture", "mileage", "doors"):
                    if min_val is not None:
                        normalized[f"{key}_min"] = min_val
                    if max_val is not None:
                        normalized[f"{key}_max"] = max_val
                    continue

                # Se for um dict sem ser range conhecido, tentar reduzir para valor direto se houver uma √∫nica chave
                if len(value) == 1:
                    only_v = next(iter(value.values()))
                    normalized[key] = only_v
                # Caso contr√°rio, ignora estruturas complexas
                continue

            # Valor plano
            if key == "doors":
                # Se vier n√∫mero √∫nico para portas, duplicar para min/max
                try:
                    doors_int = int(value)
                    normalized["doors_min"] = doors_int
                    normalized["doors_max"] = doors_int
                except Exception:
                    pass
                continue

            if key in ("year", "ano"):
                # Se for n√∫mero expl√≠cito, replicar para manufacture/model
                try:
                    year_int = int(value)
                    normalized["year_manufacture_min"] = year_int
                    normalized["year_manufacture_max"] = year_int
                    normalized["year_model_min"] = year_int
                    normalized["year_model_max"] = year_int
                    continue
                except Exception:
                    # categorias como "recente"/"antigo" ser√£o tratadas na convers√£o de prefer√™ncias
                    pass

            normalized[key] = value

        # Limpar valores None
        normalized = {k: v for k, v in normalized.items() if v is not None}
        return normalized

    def _generate_simple_question(self, missing_info: list[str]) -> str:
        """Gera pergunta simples como fallback."""
        if "marca" in missing_info:
            return "üöó Qual marca de carro voc√™ prefere?"
        elif "faixa_preco" in missing_info:
            return "üí∞ Qual sua faixa de pre√ßo?"
        elif "ano" in missing_info:
            return "üìÖ Que ano de carro voc√™ procura?"
        elif "combustivel" in missing_info:
            return "‚õΩ Qual tipo de combust√≠vel prefere?"
        elif "transmissao" in missing_info:
            return "üîß Prefere c√¢mbio manual ou autom√°tico?"
        elif "cor" in missing_info:
            return "üé® Tem alguma cor preferida?"
        elif "portas" in missing_info:
            return "üö™ Quantas portas prefere?"
        elif "quilometragem" in missing_info:
            return "üõ£Ô∏è Qual a quilometragem m√°xima que aceita?"
        else:
            return "ü§î Que outras caracter√≠sticas s√£o importantes para voc√™?"

    def get_extract_preferences_prompt(self, user_input: str) -> tuple[str, str]:
        """
        Retorna o prompt e system prompt para extra√ß√£o de prefer√™ncias.

        Args:
            user_input: Input do usu√°rio

        Returns:
            Tupla (system_prompt, user_prompt)

        """
        return (LLMPrompts.EXTRACT_PREFERENCES_SYSTEM, f"Input do usu√°rio: {user_input}\n\nExtraia as prefer√™ncias:")

    def get_generate_filters_prompt(self, preferences: dict[str, Any]) -> tuple[str, str]:
        """
        Retorna o prompt e system prompt para gera√ß√£o de filtros.

        Args:
            preferences: Prefer√™ncias do usu√°rio

        Returns:
            Tupla (system_prompt, user_prompt)

        """
        return (
            LLMPrompts.GENERATE_FILTERS_SYSTEM,
            f"Prefer√™ncias do usu√°rio: {preferences}\n\nConverta em filtros MCP:",
        )

    def get_format_results_prompt(
        self, cars: list[dict[str, Any]], user_preferences: dict[str, Any]
    ) -> tuple[str, str]:
        """
        Retorna o prompt e system prompt para formata√ß√£o de resultados.

        Args:
            cars: Lista de carros encontrados
            user_preferences: Prefer√™ncias do usu√°rio

        Returns:
            Tupla (system_prompt, user_prompt)

        """
        return (
            LLMPrompts.FORMAT_RESULTS_SYSTEM,
            f"Carros encontrados: {cars}\nPrefer√™ncias do usu√°rio: {user_preferences}\n\nApresente os resultados:",
        )

    def get_generate_question_prompt(self, preferences: dict[str, Any], missing_info: list[str]) -> tuple[str, str]:
        """
        Retorna o prompt e system prompt para gera√ß√£o de perguntas.

        Args:
            preferences: Prefer√™ncias atuais
            missing_info: Informa√ß√µes que faltam

        Returns:
            Tupla (system_prompt, user_prompt)

        """
        return (
            LLMPrompts.GENERATE_QUESTION_SYSTEM,
            f"Prefer√™ncias atuais: {preferences}\nInforma√ß√µes que faltam: {missing_info}\n\nGere uma pergunta:",
        )

    def get_generation_config(self, task: str) -> dict[str, Any]:
        """
        Retorna configura√ß√£o de gera√ß√£o para uma tarefa espec√≠fica.

        Args:
            task: Nome da tarefa (extract_preferences, generate_filters, etc.)

        Returns:
            Configura√ß√£o de gera√ß√£o

        """
        return LLMPrompts.GENERATION_CONFIGS.get(task, {"temperature": 0.7, "max_tokens": 200})

    def get_question_template(self, info_type: str) -> str:
        """
        Retorna template de pergunta para um tipo de informa√ß√£o.

        Args:
            info_type: Tipo de informa√ß√£o (marca, faixa_preco, etc.)

        Returns:
            Template da pergunta

        """
        return LLMPrompts.QUESTION_TEMPLATES.get(info_type, "Pode me dar mais detalhes sobre isso?")

    @abstractmethod
    def is_available(self) -> bool:
        """
        Verifica se o provedor LLM est√° dispon√≠vel.

        Returns:
            True se dispon√≠vel, False caso contr√°rio

        """
        pass

    @abstractmethod
    def generate_response(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 1000,
        stream: bool = False,
    ) -> str:
        """
        Gera resposta usando o modelo LLM.

        Args:
            prompt: Prompt para o modelo
            system_prompt: Prompt do sistema (opcional)
            temperature: Temperatura para gera√ß√£o (0.0 a 1.0)
            max_tokens: N√∫mero m√°ximo de tokens
            stream: Se deve usar streaming

        Returns:
            Resposta gerada pelo modelo

        """
        pass

    def extract_car_preferences(self, user_input: str) -> dict[str, Any]:
        """
        Extrair prefer√™ncias de carro do input do usu√°rio.

        Implementa√ß√£o padr√£o que pode ser sobrescrita.

        Args:
            user_input: Input do usu√°rio

        Returns:
            Dicion√°rio com prefer√™ncias extra√≠das

        """
        system_prompt, prompt = self.get_extract_preferences_prompt(user_input)
        config = self.get_generation_config("extract_preferences")

        try:
            response = self.generate_response(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=config["temperature"],
                max_tokens=config["max_tokens"],
            )

            logger.debug(f"Resposta do sistema para extract_car_preferences: {response}")

            # Tentar extrair JSON da resposta
            json_str = self._extract_json_from_response(response)
            if json_str:
                try:
                    preferences = json.loads(json_str)
                    return self._validate_preferences(preferences)
                except json.JSONDecodeError as e:
                    logger.error(f"Erro ao decodificar JSON: {e}")
                    return {}
            else:
                logger.warning("N√£o foi poss√≠vel extrair JSON da resposta")
                return {}

        except Exception as e:
            logger.error(f"Erro ao extrair prefer√™ncias: {e}")
            return {}

    def generate_car_search_filters(self, preferences: dict[str, Any]) -> dict[str, Any]:
        """
        Modificar prefer√™ncias em filtros de busca MCP.

        Implementa√ß√£o padr√£o que pode ser sobrescrita.

        Args:
            preferences: Prefer√™ncias extra√≠das do usu√°rio

        Returns:
            Filtros para busca MCP

        """
        system_prompt, prompt = self.get_generate_filters_prompt(preferences)
        config = self.get_generation_config("generate_filters")

        try:
            response = self.generate_response(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=config["temperature"],
                max_tokens=config["max_tokens"],
            )

            # Extrair JSON de forma mais robusta
            json_str = self._extract_json_from_response(response)
            if json_str:
                try:
                    filters = json.loads(json_str)
                    filters = self._normalize_llm_filters(filters)
                    return filters
                except json.JSONDecodeError as e:
                    logger.error(f"Erro ao decodificar JSON: {e}")
                    # Fallback para convers√£o direta
                    return self._convert_preferences_to_filters(preferences)
            else:
                logger.warning("Nenhum JSON encontrado na resposta, usando convers√£o direta")
                return self._convert_preferences_to_filters(preferences)

        except Exception as e:
            logger.error(f"Erro ao gerar filtros: {e}")
            # Fallback para convers√£o direta
            return self._convert_preferences_to_filters(preferences)

    def format_car_results(self, cars: list[dict[str, Any]], user_preferences: dict[str, Any]) -> str:
        """
        Formatar resultados de busca de carros.

        Implementa√ß√£o padr√£o que pode ser sobrescrita.

        Args:
            cars: Lista de carros encontrados
            user_preferences: Prefer√™ncias do usu√°rio

        Returns:
            Resultados formatados

        """
        if not cars:
            return "üòî N√£o encontrei carros que atendam aos seus crit√©rios. Que tal ajustarmos a busca?"

        # Preparar dados simplificados para a LLM
        simplified_cars = self._simplify_cars_for_formatting(cars[:5])  # Limitar a 5 carros

        logger.info(f"Carros formatados com sucesso: {len(simplified_cars)} carros encontrados")

        # Usar prompts centralizados
        system_prompt, prompt = self.get_format_results_prompt(simplified_cars, user_preferences)
        config = self.get_generation_config("format_results")

        try:
            response = self.generate_response(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=config["temperature"],
                max_tokens=config["max_tokens"],
            )
            logger.info(f"Resposta gerada com sucesso: {len(response)} caracteres")
            return response

        except Exception as e:
            logger.error(f"Erro ao formatar resultados: {e}")
            # Fallback para formata√ß√£o simples
            return self._format_cars_simple(cars)

    def generate_next_question(self, current_preferences: dict[str, Any], missing_info: list[str]) -> str:
        """
        Gerar pr√≥xima pergunta baseada no contexto.

        Implementa√ß√£o padr√£o que pode ser sobrescrita.

        Args:
            current_preferences: Prefer√™ncias j√° coletadas
            missing_info: Informa√ß√µes que ainda faltam

        Returns:
            Pr√≥xima pergunta sugerida

        """
        if not missing_info:
            return "Perfeito! Tenho informa√ß√µes suficientes para buscar carros para voc√™."

        # Usar prompts centralizados
        system_prompt, prompt = self.get_generate_question_prompt(current_preferences, missing_info)
        config = self.get_generation_config("generate_question")

        try:
            response = self.generate_response(
                prompt=prompt,
                system_prompt=system_prompt,
                temperature=config["temperature"],
                max_tokens=150,  # Limite baixo para perguntas concisas
            )
            logger.debug(f"Resposta do sistema para next question: {response}")
            return response

        except Exception as e:
            logger.error(f"Erro ao gerar pergunta: {e}")
            # Fallback para perguntas simples
            return self._generate_simple_question(missing_info)
